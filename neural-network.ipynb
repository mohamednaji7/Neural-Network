{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/naji468/neural-network?scriptVersionId=97322003\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# Content\n* Neuarl network model: \n  * 1-Paramters Inilization \n  * 2-Forward Propogation  \n  * 3-Backworad Propogation  \n  * 4-Update Parameters  \n  * put our model in a calss   \n* Test Our Model(use it as 1-calss clssifer): against cat and not a cat dataset   \n  * explore the data set  \n  * train  \n  * predict  \n  * anayze  \n* Test the model with random images from the internet  \n* next?   \nimprove our deep neural network with hyperparameters and regularization   \ni will made a multi-class classifier using our NN model   \nand test with MNIST dataset  ","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport h5py \nfrom sklearn.metrics import mean_squared_error\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-02T15:57:48.129357Z","iopub.execute_input":"2022-06-02T15:57:48.130073Z","iopub.status.idle":"2022-06-02T15:57:48.173482Z","shell.execute_reply.started":"2022-06-02T15:57:48.130035Z","shell.execute_reply":"2022-06-02T15:57:48.172123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build a Neuarl Netword L-Layer Model","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://programmer.ink/images/think/b5ff0810ec6aab30c9f5ed5846e3cb86.jpg\" style=\"width:1200px;height:600px;\">","metadata":{}},{"cell_type":"markdown","source":"## 1-Paramters Inilization","metadata":{}},{"cell_type":"code","source":"def initialize_parameters_deep(layer_dims):\n    \"\"\"\n    Arguments:\n    layer_dims -- python array (list) containing the dimensions of each layer in our network\n    \n    Returns:\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n                    bl -- bias vector of shape (layer_dims[l], 1)\n    \"\"\"\n    np.random.seed(1)\n\n    L = len(layer_dims)\n    parameters = {}\n    for l in range(1, L):\n        parameters['W'+str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1])#*0.01, \n        #if we get a more i/p for a layer then it's weights should be small to consider the the i/ps\n        #if it less >> higher wiegths \n        parameters['b'+str(l)] = np.zeros((layer_dims[l], 1))\n        \n    return parameters","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:57:48.284579Z","iopub.execute_input":"2022-06-02T15:57:48.285434Z","iopub.status.idle":"2022-06-02T15:57:48.329216Z","shell.execute_reply.started":"2022-06-02T15:57:48.285389Z","shell.execute_reply":"2022-06-02T15:57:48.327902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2 - Forward Propogation","metadata":{}},{"cell_type":"markdown","source":"the pre-activation o/p of a l-layer is $Z^{[l]}=W^{[l]}A^{[l]}+b^{[l]}$","metadata":{}},{"cell_type":"code","source":"def linear_forward(A_prev, W, b):\n    \"\"\"\n    Implement the linear part of a layer's forward propagation.\n\n    Arguments:\n    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n    b -- bias vector, numpy array of shape (size of the current layer, 1)\n\n    Returns:\n    Z -- the input of the activation function, also called pre-activation parameter \n    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n    \"\"\"\n    Z = np.dot(W, A_prev) + b\n    cache = (A_prev, W, b)\n    \n    return Z, cache","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:57:48.465714Z","iopub.execute_input":"2022-06-02T15:57:48.466162Z","iopub.status.idle":"2022-06-02T15:57:48.510427Z","shell.execute_reply.started":"2022-06-02T15:57:48.466127Z","shell.execute_reply":"2022-06-02T15:57:48.509597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- neural network with hidden layers as a linear acivation does a neural network it's nested linear regression function, to get the sence of the neural network as propgated predictions we use a an non-linear activation function for the hidden layers \n- linear -> activation forward:\n<br/>\nmost common activation function is the Sigmoid and ReLU Function\n-**ReLU**: $A = ReLU(Z) = max(0, Z)$. <br/>\nthis relu works will as it has a linear regression that make the dervitivae of it is 1\n-**Sigmoid**:$\\Sigma(Z) = \\frac{1}{1+e^{-(Z)}}$ <br/>\nthis as linear activation is perfect or making the bounder of the o/p between {1, 0}\n\n<img src=\"https://www.researchgate.net/publication/352419028/figure/fig3/AS:1035156496338953@1623811949112/Sigmoid-and-ReLU-functions-In-order-to-optimize-the-model-forward-propagation-and-back.ppm\" style=\"width:600px;height:300px;\">","metadata":{}},{"cell_type":"code","source":"def sigmoid(Z):\n    cache = Z\n    A = 1/(1+np.exp(-Z))\n    \n    return A, cache\n\ndef relu(Z):\n    cache = Z \n    A = np.maximum(0,Z)\n        \n    return A, cache","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:57:48.627007Z","iopub.execute_input":"2022-06-02T15:57:48.628171Z","iopub.status.idle":"2022-06-02T15:57:48.669026Z","shell.execute_reply.started":"2022-06-02T15:57:48.628116Z","shell.execute_reply":"2022-06-02T15:57:48.667944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def linear_activation_forward(A_prev, W, b, activation):\n    \"\"\"\n    Implement the forward propagation for the LINEAR->ACTIVATION layer\n\n    Arguments:\n    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n    b -- bias vector, numpy array of shape (size of the current layer, 1)\n    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n\n    Returns:\n    A -- the output of the activation function, also called the post-activation value \n    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n             stored for computing the backward pass efficiently\n    \"\"\"\n    Z, linear_cache = linear_forward(A_prev, W, b)#A_prev,W,b\n    if activation==\"sigmoid\":\n        A, activation_cache = sigmoid(Z)#Z\n    elif activation == \"relu\":\n        A, activation_cache = relu(Z)#Z\n    cache = (linear_cache, activation_cache)\n    \n    return A, cache","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:57:48.797549Z","iopub.execute_input":"2022-06-02T15:57:48.798231Z","iopub.status.idle":"2022-06-02T15:57:48.840853Z","shell.execute_reply.started":"2022-06-02T15:57:48.798177Z","shell.execute_reply":"2022-06-02T15:57:48.839694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Forward Propogation Function\nwe do L-1 linear->activation(relu) and 1 linear->activation(sigmoid)<br/>\ngat the $\\hat{Y} = A^{[L]}$  \n<img src=\"https://cemsarier.github.io/images/perceptron/model_arch_fw.png\" style=\"width:1200px;height:500px;\">","metadata":{}},{"cell_type":"code","source":"def forward_propagation(X, parameters):\n    \"\"\"\n    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n    \n    Arguments:\n    X -- data, numpy array of shape (input size, number of examples)\n    parameters -- output of initialize_parameters_deep()\n    \n    Returns:\n    AL -- activation value from the output (last) layer\n    caches -- list of caches containing:\n                every cache of linear_activation_forward() (there are L of them, indexed from 0 to L-1)\n    \"\"\"\n    caches=[]\n    A=X\n    L=len(parameters)//2\n    #linear->relu\n    for l in range(1,L):\n        A_prev= A\n        A,cache = linear_activation_forward(A_prev, parameters['W'+str(l)], parameters['b'+str(l)], \"relu\")\n        caches.append(cache)#  A_prev,W,b  ,,  Z\n        \n    AL, cache = linear_activation_forward(A, parameters['W'+str(L)], parameters['b'+str(L)], \"sigmoid\")\n    caches.append(cache)\n    \n    return AL, caches","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:57:48.957548Z","iopub.execute_input":"2022-06-02T15:57:48.958635Z","iopub.status.idle":"2022-06-02T15:57:49.001464Z","shell.execute_reply.started":"2022-06-02T15:57:48.958579Z","shell.execute_reply":"2022-06-02T15:57:49.000423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cost Function\nCompute the cross-entropy cost $J$: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))Â \\tag{1}$$","metadata":{}},{"cell_type":"code","source":"def compute_cost(AL, Y):\n    \"\"\"\n    Implement the cost function defined by equation (7).\n\n    Arguments:\n    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n\n    Returns:\n    cost -- cross-entropy cost\n    \"\"\"\n    \n    m = Y.shape[1]\n    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n    \n    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n    assert(cost.shape == ())\n    \n    return cost","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:57:49.128306Z","iopub.execute_input":"2022-06-02T15:57:49.12903Z","iopub.status.idle":"2022-06-02T15:57:49.171483Z","shell.execute_reply.started":"2022-06-02T15:57:49.128977Z","shell.execute_reply":"2022-06-02T15:57:49.170699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2 - Backward Propgation","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://discuss.pytorch.org/uploads/default/original/3X/4/d/4d34f014ede7d5bd594815a880ab509a53c96024.png\"/>","metadata":{}},{"cell_type":"markdown","source":" ### You want to get $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$.\n \\#Suppose you have already calculated the derivative $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$\n\n\\->For layer $l$, the linear part is: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (followed by an activation).\n\nHere are the formulas you need:\n$$ dW^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} $$\n$$ db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$\n$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{2}$$","metadata":{}},{"cell_type":"code","source":"\ndef linear_backward(dZ, linear_cache):\n    \"\"\"\n    Implement the linear portion of backward propagation for a single layer (layer l)\n\n    Arguments:\n    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n    linear_cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n\n    Returns:\n    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n    \"\"\"\n    A_prev, W, b = linear_cache\n    m = A_prev.shape[1]\n    \n    dW = np.dot(dZ, A_prev.T)/m\n    db = np.sum(dZ, axis=1, keepdims=True)/m\n    dA_prev = np.dot(W.T, dZ)\n    \n    return dA_prev, dW, db","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:57:49.304601Z","iopub.execute_input":"2022-06-02T15:57:49.305064Z","iopub.status.idle":"2022-06-02T15:57:49.348596Z","shell.execute_reply.started":"2022-06-02T15:57:49.305028Z","shell.execute_reply":"2022-06-02T15:57:49.347229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ### You want to get $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$\n\\#Suppose you have already calculated the derivative $A^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l]}}$\n\\-> $A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$ where the activation \"g\" can be sigmoid() or relu().\n\n$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}). $$  \n","metadata":{}},{"cell_type":"markdown","source":"$ReLU'(Z^{[l]}) = 0, Z^{[l]}<0$\n\n$ReLU'(Z^{[l]}) = 1, else$\n\n$  \\sigma(s)' = \\sigma(s) * (1-\\sigma(s))$","metadata":{}},{"cell_type":"code","source":"\ndef relu_backward(dA, cache):\n    Z = cache\n    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n    # When z <= 0, you should set dz to 0 as well. \n    dZ[Z <= 0] = 0\n    return dZ\n\ndef sigmoid_backward(dA, cache):\n    Z = cache    \n    s = 1/(1+np.exp(-Z))\n    dZ = dA * s * (1-s)\n    return dZ","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:57:49.472555Z","iopub.execute_input":"2022-06-02T15:57:49.473374Z","iopub.status.idle":"2022-06-02T15:57:49.516084Z","shell.execute_reply.started":"2022-06-02T15:57:49.47333Z","shell.execute_reply":"2022-06-02T15:57:49.514929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def linear_activation_backward(dA, cache, activation):\n    \"\"\"\n    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n    \n    Arguments:\n    dA -- post-activation gradient for current layer l \n    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n    \n    Returns:\n    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n    \"\"\"\n    linear_cache, activation_cache  = cache\n    if activation == \"relu\":\n        dZ = relu_backward(dA, activation_cache)\n    elif  activation == \"sigmoid\":\n        dZ = sigmoid_backward(dA, activation_cache) \n    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n    \n    return dA_prev, dW, db","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:57:49.670436Z","iopub.execute_input":"2022-06-02T15:57:49.670917Z","iopub.status.idle":"2022-06-02T15:57:49.714952Z","shell.execute_reply.started":"2022-06-02T15:57:49.670879Z","shell.execute_reply":"2022-06-02T15:57:49.713769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ### You want to get $A^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l]}}$\n For layer $l$, \n \n you get it from the o/p of the layer $l+1$, equation (1)\n\n For layer $L$, \n \n you compute $\\frac{\\partial \\mathcal{L} }{\\partial A^{[L]}}$\n \n from (1)\n ```python\ndAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n```","metadata":{}},{"cell_type":"markdown","source":"### Backward Propogation Function","metadata":{}},{"cell_type":"code","source":"def backward_propogation(AL, Y, caches):\n    \"\"\"\n    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n    \n    Arguments:\n    AL -- probability vector, output of the forward propagation (forward_propogation())\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n    caches -- list of caches containing:\n                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n    \n    Returns:\n    grads -- A dictionary with the gradients\n             grads[\"dA\" + str(l)] = ... \n             grads[\"dW\" + str(l)] = ...\n             grads[\"db\" + str(l)] = ... \n    \"\"\"\n    #first dAL, the backward propgate to the 0-layer\n    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n    grades={}\n    m = AL.shape[1]\n    L = len(caches)\n    Y = Y.reshape( AL.shape)\n    \n    #cache the L-layer in index l-1\n    L_cache = caches[L-1]\n    dA_prev_L, dW_L, db_L = linear_activation_backward(dAL, L_cache, \"sigmoid\")\n    grades[\"dA\"+str(L-1)] = dA_prev_L\n    grades[\"dW\"+str(L)] = dW_L\n    grades[\"db\"+str(L)] = db_L\n    #dA-1 \n    #layer L gradients saved, good job\n    \n    for l in reversed(range(L-1)): \n        #index = l : 0,1,..,L-2 #reversed-> L-2, L-3, ....,0\n        #layers: L-1, L-2, ...., 1\n        l_cache = caches[l]\n        # you get it from the o/p of the layer $l+1$, equation (1)\n        dA_prev_l, dW_l, db_l = linear_activation_backward(grades[\"dA\"+str(l+1)], l_cache, \"relu\")\n        #layers: L-1, L-2, ...., 1\n        grades[\"dA\"+str(l)] = dA_prev_l\n        grades[\"dW\"+str(l+1)] = dW_l\n        grades[\"db\"+str(l+1)] = db_l\n    \n    return grades        ","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:57:49.845715Z","iopub.execute_input":"2022-06-02T15:57:49.846676Z","iopub.status.idle":"2022-06-02T15:57:49.89348Z","shell.execute_reply.started":"2022-06-02T15:57:49.846635Z","shell.execute_reply":"2022-06-02T15:57:49.892277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4-Update Parameters","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def update_parameters(params, grads, learning_rate):\n    \"\"\"\n    Update parameters using gradient descent\n    \n    Arguments:\n    params -- python dictionary containing your parameters \n    grads -- python dictionary containing your gradients, output of L_model_backward\n    \n    Returns:\n    parameters -- python dictionary containing your updated parameters \n                  parameters[\"W\" + str(l)] = ... \n                  parameters[\"b\" + str(l)] = ...\n    \"\"\"\n    parameters = params.copy()\n    L = len(parameters) // 2 # number of layers in the neural network\n\n    for l in range(L):\n        parameters[\"W\"+str(l+1)]=parameters[\"W\"+str(l+1)]-learning_rate*grads[\"dW\"+str(l+1)]\n        parameters[\"b\"+str(l+1)]=parameters[\"b\"+str(l+1)]-learning_rate*grads[\"db\"+str(l+1)]\n    \n    return parameters","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:57:50.022895Z","iopub.execute_input":"2022-06-02T15:57:50.023306Z","iopub.status.idle":"2022-06-02T15:57:50.066678Z","shell.execute_reply.started":"2022-06-02T15:57:50.023274Z","shell.execute_reply":"2022-06-02T15:57:50.065599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Build a L-Layer Neuarl Network Model in a Class","metadata":{}},{"cell_type":"code","source":"class NeuralNetwork():\n    \"\"\"\n    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n    \n    Arguments:\n    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n    learning_rate -- learning rate of the gradient descent update rule\n    num_iterations -- number of iterations of the optimization loop\n    print_cost -- if True, it prints the cost every 100 steps\n    \"\"\"\n    def __init__(self, layers_dims, learning_rate, num_iterations):\n        self.layers_dims = layers_dims\n        self.learning_rate = learning_rate\n        self.num_iterations = num_iterations\n        # 1 - Parameters initialization.\n        self.parameters = initialize_parameters_deep(self.layers_dims)\n        \n    def fit(self, X, Y):\n        \"\"\"\"\n        Arguments:\n        X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n        Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n        \"\"\"\n        for i in range(self.num_iterations):\n            # 2 - Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n            parameters = self.parameters.copy()\n            AL, caches = forward_propagation(X, parameters)\n            \n            \n            # 3 - Backward propagation.\n            grads = backward_propogation(AL, Y, caches)\n\n            # 4 - Update parameters.\n            self.parameters = update_parameters(parameters, grads, self.learning_rate)\n            \n        \n    def predict(self, X):\n        \"\"\"\n        Arguments:\n        X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n        \"\"\"\n                \n        m = X.shape[1]\n        parameters = self.parameters.copy()\n        \n        probas, caches =  forward_propagation(X, parameters)\n        \n        return probas    ","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:57:50.206549Z","iopub.execute_input":"2022-06-02T15:57:50.206971Z","iopub.status.idle":"2022-06-02T15:57:50.2524Z","shell.execute_reply.started":"2022-06-02T15:57:50.206937Z","shell.execute_reply":"2022-06-02T15:57:50.251101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test Our Model: #against cat and not a cat dataset","metadata":{}},{"cell_type":"markdown","source":"## explore the data set","metadata":{}},{"cell_type":"markdown","source":"### load the data","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\nplt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n\n%load_ext autoreload\n%autoreload 2","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:57:50.399367Z","iopub.execute_input":"2022-06-02T15:57:50.400101Z","iopub.status.idle":"2022-06-02T15:57:50.446254Z","shell.execute_reply.started":"2022-06-02T15:57:50.400052Z","shell.execute_reply":"2022-06-02T15:57:50.4448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_dataset():\n    train_dataset = h5py.File('../input/cat-and-not-cat/train_catvnoncat.h5', \"r\")\n    train_set_x_orig = np.array(train_dataset[\"train_set_x\"]) # your train set features\n    train_set_y_orig = np.array(train_dataset[\"train_set_y\"]) # your train set labels\n\n    test_dataset = h5py.File('../input/cat-and-not-cat/test_catvnoncat.h5', \"r\")\n    test_set_x_orig = np.array(test_dataset[\"test_set_x\"]) # your test set features\n    test_set_y_orig = np.array(test_dataset[\"test_set_y\"]) # your test set labels\n\n    classes = np.array(test_dataset[\"list_classes\"]) # the list of classes\n\n    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n\n    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:57:50.584644Z","iopub.execute_input":"2022-06-02T15:57:50.585262Z","iopub.status.idle":"2022-06-02T15:57:50.627782Z","shell.execute_reply.started":"2022-06-02T15:57:50.585225Z","shell.execute_reply":"2022-06-02T15:57:50.626797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x_orig, train_y, test_x_orig, test_y, classes = load_dataset()\n","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:57:50.833748Z","iopub.execute_input":"2022-06-02T15:57:50.835201Z","iopub.status.idle":"2022-06-02T15:57:50.951638Z","shell.execute_reply.started":"2022-06-02T15:57:50.835145Z","shell.execute_reply":"2022-06-02T15:57:50.950301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## inspect 1 picture","metadata":{}},{"cell_type":"code","source":"# Example of a picture\nindex = 10\nplt.imshow(train_x_orig[index])\nprint (\"y = \" + str(train_y[0,index]) + \". It's a \" + classes[train_y[0,index]].decode(\"utf-8\") +  \" picture.\")","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:57:51.03591Z","iopub.execute_input":"2022-06-02T15:57:51.036354Z","iopub.status.idle":"2022-06-02T15:57:51.215372Z","shell.execute_reply.started":"2022-06-02T15:57:51.036318Z","shell.execute_reply":"2022-06-02T15:57:51.214596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Explore your dataset \nm_train = train_x_orig.shape[0]\nnum_px = train_x_orig.shape[1]\nm_test = test_x_orig.shape[0]\n\nprint (\"Number of training examples: \" + str(m_train))\nprint (\"Number of testing examples: \" + str(m_test))\nprint (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\nprint (\"train_x_orig shape: \" + str(train_x_orig.shape))\nprint (\"train_y shape: \" + str(train_y.shape))\nprint (\"test_x_orig shape: \" + str(test_x_orig.shape))\nprint (\"test_y shape: \" + str(test_y.shape))","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:57:51.27777Z","iopub.execute_input":"2022-06-02T15:57:51.278458Z","iopub.status.idle":"2022-06-02T15:57:51.325616Z","shell.execute_reply.started":"2022-06-02T15:57:51.278414Z","shell.execute_reply":"2022-06-02T15:57:51.324506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reshape the training and test examples \ntrain_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\ntest_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n\n# Standardize data to have feature values between 0 and 1.\ntrain_x = train_x_flatten/255.\ntest_x = test_x_flatten/255.\n\nprint (\"train_x's shape: \" + str(train_x.shape))\nprint (\"test_x's shape: \" + str(test_x.shape))\nprint (\"train's percentage of the data: {:.0f}%\".format( 100*train_x.shape[1]/(train_x.shape[1]+test_x.shape[1])))\nprint (\"test's percentage of the data: {:.0f}%\".format( 100*test_x.shape[1]/(train_x.shape[1]+test_x.shape[1])))","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:57:51.471409Z","iopub.execute_input":"2022-06-02T15:57:51.472543Z","iopub.status.idle":"2022-06-02T15:57:51.527419Z","shell.execute_reply.started":"2022-06-02T15:57:51.472495Z","shell.execute_reply":"2022-06-02T15:57:51.526202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"layers_dims = [12288, 20, 7, 5, 1] #  4-layer model\n\nmodel = NeuralNetwork(layers_dims, learning_rate = 0.0075, num_iterations = 3000)\n    \nmodel.fit(train_x, train_y)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:57:51.850144Z","iopub.execute_input":"2022-06-02T15:57:51.850942Z","iopub.status.idle":"2022-06-02T15:59:17.741114Z","shell.execute_reply.started":"2022-06-02T15:57:51.850889Z","shell.execute_reply":"2022-06-02T15:59:17.739637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predict","metadata":{}},{"cell_type":"code","source":"pred_train = model.predict(train_x)\npred_test = model.predict(test_x)\npred_test","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:59:17.743675Z","iopub.execute_input":"2022-06-02T15:59:17.744737Z","iopub.status.idle":"2022-06-02T15:59:17.833429Z","shell.execute_reply.started":"2022-06-02T15:59:17.744678Z","shell.execute_reply":"2022-06-02T15:59:17.832371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_train[pred_train>=0.5]=1\npred_train[pred_train<0.5]=0\n\npred_test[pred_test>=0.5]=1\npred_test[pred_test<0.5]=0\npred_test","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:59:17.83529Z","iopub.execute_input":"2022-06-02T15:59:17.836077Z","iopub.status.idle":"2022-06-02T15:59:17.908254Z","shell.execute_reply.started":"2022-06-02T15:59:17.836028Z","shell.execute_reply":"2022-06-02T15:59:17.90682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Analyze","metadata":{}},{"cell_type":"code","source":"print(\"Accurecy of train data: {:.2f}%\".format(100*np.sum(pred_train==train_y)/train_y.size))\nprint(\"Accurecy of test data: {}%\".format(100*np.sum(pred_test==test_y)/test_y.size))","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:59:17.911867Z","iopub.execute_input":"2022-06-02T15:59:17.913191Z","iopub.status.idle":"2022-06-02T15:59:17.968878Z","shell.execute_reply.started":"2022-06-02T15:59:17.913133Z","shell.execute_reply":"2022-06-02T15:59:17.967728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test the model with random images from the internet","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nimport requests\nfrom io import BytesIO\n\nurl1 = \"https://bluebuffalo.com/globalassets/00-redesign/articles/cat-articles/how-to-introduce-two-cats/01_adding_cat_body1.jpg\"\nurl2 = \"https://tvnz-1-news-prod.cdn.arcpublishing.com/resizer/pBHCGaEZv6-KtsIRxoiOpGU133M=/800x450/filters:format(jpg):quality(70):focal(-5x-5:5x5)/cloudfront-ap-southeast-2.images.arcpublishing.com/tvnz/SOYANY2MQ5CSVFOTG6NXOWTSXY.jpg\"\nurl3 = \"https://www.dmarge.com/wp-content/uploads/2021/01/dwayne-the-rock-.jpg\"\nurls = [url1, url2, url3]\n\n\nimages = np.zeros((num_px * num_px * 3, len(urls)))\nimages = []\nfor i in range(len(urls)):\n    response = requests.get(urls[i])\n    img = Image.open(BytesIO(response.content))\n    image = np.array(img.resize((num_px, num_px)))\n    image = image / 255.\n    plt.figure()\n    plt.imshow(image)\n    image = image.reshape((1, num_px * num_px * 3)).T\n    images.append(image)\nimages = np.squeeze(np.asarray(images).T)\n\n\nmy_predicted_images = model.predict(images)\nfor i in range(my_predicted_images.shape[1]):\n    print (\"y = \" + str(np.squeeze(my_predicted_images[0][i])) + \",  L-layer model predicts a \\\"\" + classes[int(my_predicted_images[0][i]+0.5)].decode(\"utf-8\") + \"\\\" picture.\")","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:59:17.97053Z","iopub.execute_input":"2022-06-02T15:59:17.971019Z","iopub.status.idle":"2022-06-02T15:59:23.574579Z","shell.execute_reply.started":"2022-06-02T15:59:17.970974Z","shell.execute_reply":"2022-06-02T15:59:23.573544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# Next?  \nimprove our deep neural network with hyperparameters and regularization  \ni will made a multi-class classifier using our NN model  \nand test with MNIST dataset","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}